---
title: "mice example code"
author: "Maria Sanchez and Marta Pineda-Moncusi"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

/\* Whole document: \*/ body{ font-family: Helvetica; font-size: 16pt; }

```{r}
rm(list=ls())
```

```{r setup, include=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tutorial for multiple imputation in R

The purpose of this practical is to discuss commonly used techniques for
handling missing data and common issues that could arise when these
techniques are used. We will focus on one of the most popular methods,
multiple imputation.

We will use the data set ***hip.csv***. This data set is made up for
this practical. The R code for this practical was developed using R
4.2.1 version.

## Load and evaluate the data set

**The Hip Replacement (HIP) data set**

The hip data set contains information on seven hundred and eight
patients receiving primary hip replacement surgery for osteoarthritis
(variable `id` is the unique patient identifier). Prior to the
operation, patients completed a pre-operative Oxford Hip Score (OHS) and
EQ5D (Euroqol) questionnaire with a follow-up questionnaire being filled
in at 6-months post-surgery. The OHS consists of 12 questions asking
patients to describe their hip pain and function during the past 4
weeks. An overall score is created by summing the responses to each of
the 12 questions, ranging from 0 to 48, where 0 is the worst possible
score (severe symptoms) and 48 the best score (excellent joint
function). Variable `ohs0` is the preoperative score and `ohs6`
post-operative. The absolute change in OHS between pre- and
post-operative assessments (variable `ohsdiff`) is negative if patient
symptoms have improved and positive for worsening.

The pre-operative `eq5d0` contains information from 5 questions asking
about a patient's health state today, covering mobility, self-care,
usual activities, pain, and anxiety. The `eq5d0` has been converted to a
single summary score (variable `eq5d0`), anchored at 0 for death and 1
for full health, with some health states being worse than dead (-0.594).

Six months after their operation patients were asked about their overall
satisfaction with the outcome of surgery measured on a visual analogue
scale from 0 to 100 (variable `satisfaction`).

Pre-operative information was collected on age at the time of surgery,
`sex` (0 = Male; 1 = Female), `height` (metres) and `weight` (kg) (from
which body mass index (`bmi`) is calculated), `side` of surgery (Left;
Right), `ethnic` group (0 = White; 1 = Non white), whether or not they
are `retired` (0 = Not retired; 1 = Retired). The Index of Multiple
Deprivation is a measure of social deprivation, linked to the area a
patient lives in (variable `imdscore`).

-   The outcome/dependent variable (*Y*) for this study is `ohsdiff` =
    change in ohs before and after surgery

-   The independent variables (*X's*) are = `age`, `sex`, `bmi`,
    `ethnic`, `imdscore`

```{r dplyr, results='hide', warning = FALSE, message = FALSE}

#install.packages("dplyr")
library(dplyr)

#See your working directory
getwd()

#Update your working directory to the folder where "hip.data" is stored (if necessary)
update_wd = "NO" #"NO" for keeping the current working directory. "YES" to update, plus including the new path in folder_path variable below.
if(update_wd == "YES"){
folder_path <- ("C:/Users/msanchez/Documents/GitHub/Practical_Imputation/multiple-imputation_with_mice/")  #Remind to use / (NOT \)
setwd(folder_path)}

#load data
data <- read.csv("hip.csv", sep=',')

#MAR smoking variable
data$smoking[data$sex == 1] = 
  sample(c("Smoker", "Non-smoker", NA), 
         sum(data$sex == 1), 
         replace = TRUE,
         prob = c(0.1, 0.5, 0.4))

data$smoking[data$sex == 0] = 
  sample(c("Smoker", "Non-smoker", NA), 
         sum(data$sex == 0), 
         replace=TRUE, prob = c(0.15, 0.75, 0.1))
data$smoking = factor(data$smoking)

#occupation correlated to outcom
data$occupation = ifelse(data$ohsdiff>median(data$ohsdiff), 1 ,0)
occupation = factor(occupation, levels = c(0,1), labels = c("Manual worker","Non-manual worker"))

                    
#Display the names of variables in the data set.
names(data)

#transform/include labels to data
data = data %>% 
            mutate(sex = factor(sex, levels = c(0,1), labels = c("Male","Female")),
                 retired = factor(retired, levels = c(0,1), labels = c("Not retired","Retired")),
                 ethnic = factor(ethnic, levels = c(0,1), labels = c("White","Non white")),
                 side = factor(side, levels = c(0,1), labels = c("Right","Left")))
                 smoking = factor(smoking, levels = c(0,1), labels = c("Smoker","Non-smoker"))

#Observe/explore the data
dim(data)
summary(data)
```

Check the structure of the data:

-   How many cases (individuals) are in the data? 708

-   How many variables do we have? 16

-   Which are numeric? age, oh0, oh6, ohsdiff, EQ5D0, height, weight,
    satisfaction, bmi and imdscore

-   Which are factor? sex, retired, ethnic, side and smoking

-   Is there any variable with missing? EQ5D0, height, weight, bmi,
    satisfaction, ethnic, imdscore and smoking

-   Describe the outcome: Mean (-18.6), Median (Range): -19 (-43-19).
    Mean and median are very close, we can think that the distribution
    of the outcome is normal. You would like to check this assumption
    with a density plot or histogram. The majority of patients improved
    in terms of pain and function after surgery (negative values).

    ------------------------------------------------------------------------

## 1. Exploring missing data mechanisms

-   Missing completely at random (MCAR): The probability of data being
    missing is the same for all cases (not depend on the observed or
    unobserved data)

-   Missing at random (MAR): The probability of data being missing is
    the same only within groups defined by the observed data

-   Missing not at random (MNAR): The probability of data being missing
    varies for reasons that are unknown to us or does depend on the
    unobserved data, conditional on the observed data

![](images/Types%20of%20missing%20data.png)

**Check that variables with missing data are associated with variables
that are complete**

To do so, create a dummy variable were missing values are 1 and complete
values are 0, and run a logistic regression.

```{r, results='hide', warning = FALSE}
#list the variables you want to test
test_var_list = c("eq5d0", "height", "weight", "bmi", "satisfaction", "ethnic", "imdscore")
complete_var  = c("age", "sex", "retired", "ohsdiff","side")

for (v in 1:length(test_var_list)){
 test_var = ifelse(is.na(data[[test_var_list[v]]]), 1,0 )
 test = glm(test_var ~ ., data=data[complete_var] )
  print(paste("Tested variable: ", test_var_list[v], sep="")) 
  print(summary(test))   #Check p_value
}
```

Alternative, we can use the function na.test() from the package misty:
This function tests all your dataset. If the output is a significant p
value (p\<0.05), you can reject the null hypothesis that the data is "Missing Completely at
Random" (MCAR) and you can impute. If p\>0.05, you can't reject the null hypothesis, therefore, your data might be MCAR
unless you think is possible/suspect that a variable not recorded in
your data could explain the missingness.

<https://search.r-project.org/CRAN/refmans/misty/html/na.test.html#>:\~:text=Little%20(1988)%20proposed%20a%20multivariate,with%20the%20expected%20population%20means
<https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0442-1#ref-CR1>

```{r, misty}
library(misty)

na.test(data)

```

------------------------------------------------------------------------

## 2. Missing value patterns

Types of missing data patterns:[![Some missing data patterns in
multivariate data. Blue is observed, red is
missing.](images/missing%20value%20patterns.png){width="628"}](https://stefvanbuuren.name/fimd/about-the-author.html)\
*Source: Flexible Imputation of Missing Data, Second Edition. Author:
Stefvan Buuren*

Some missing data patterns in multivariate data. Blue is observed, red
is missing:

1.  *Univariate and multivariate*. A missing data pattern is said to be
    *univariate* if there is only one variable with missing data.
2.  *Monotone and non-monotone (or general)*. A missing data pattern is
    said to be *monotone* when, if the variables are ordered from least
    to most missing, then all observations of a variable contain missing
    in the observations in which the prior variable contains missing
    (*it can occur as a result of drop-out in longitudinal studies*). If
    the pattern is not monotone, it is called *non-monotone or general*
3.  *Connected and unconnected*. A missing data pattern is said to be
    *connected* if any observed data point can be reached from any other
    observed data point through a sequence of horizontal or vertical
    moves (like the rook in chess).

The missing data pattern influences the amount of information that can
be transferred between variables.

Imputation can be more precise if other variables are non-missing for
those cases that are to be imputed. The reverse is also true. Predictors
are potentially more powerful if they have are non-missing in rows that
are vastly incomplete. This section discusses various measures of the
missing data pattern.

The mice package contains a function to inspect the missing data
pattern: `md.pattern()`

```{r mice, warning = FALSE}

#Selecting variables with missing data
missvars <- c("imdscore", "eq5d0", "satisfaction", "ethnic", "height", "weight", "bmi", "retired", "smoking", "occupation")
missdata <- data[missvars]

#Exploring missing pattern with md.pattern() function
#The md.pattern() function shipped with Multivariate Imputation by Chained Equations (MICE) package can be used to produce a table displaying the missing pattern.

#install.package("mice")
library(mice)
md.pattern(missdata, plot=FALSE)

```

-   Can you please interpreter the output table? what do you think it is
    displaying?

**Interpreation of the table In the main body of the output table** 
The values "1" indicates non-missing value and "0" indicates missing value.
The first column shows the number of unique missing data patterns.

There are 330 observations with non-missing values, and there are 227
observations with non-missing values except for the variable 'retired'.
The rightmost column shows the number of missing variables in a
particular missing pattern. For example, the first row has no missing
value and it is "0" in the row. The last row counts the number of
missing values for each variable. For example, the variable
***imdscore*** contains 11 missing values and the variable ***bmi***
contains 359 missing values. This table can be helpful when you decide
to drop some observations with missing variables exceeding a preset
threshold.

```{r ggmice VIM, warning = FALSE}
#Visual presentation of missing data pattern

#We are using three different functions to display visually the patterns of the missing data in R: 

#library(mice)
md.pattern(missdata, plot=TRUE, rotate.names=TRUE)

#install.package("ggmice")
library(ggmice)
plot_pattern(missdata, vrb = "all", square = FALSE, rotate = TRUE, cluster = NULL)

#install.packages("VIM")
library(VIM)
aggr(missdata, labels=names(data), numbers=FALSE, prop=TRUE)

```

See more at: [Chapter 4 Multivariate missing
data](https://stefvanbuuren.name/fimd/ch-multivariate.html){.uri}

-   What type of missing data pattern you think we have in our data set?

    ------------------------------------------------------------------------

## 3. Checking linear relationship between numeric variables and outcome.

```{r, results='hide', warning = FALSE, fig.show='hide'}
#list the variables you want to test
test_var_list = c("age", "eq5d0", "height", "weight", "satisfaction", "bmi", "imdscore" )
outcome = "ohsdiff"

for (v in 1:length(test_var_list)){
  df = cbind(data[test_var_list[v]],outcome = data[[outcome]])
  #df= as.data.frame(df) #remove tibble
  test = lm(outcome ~ ., data = df)
  par(mfrow = c(2, 2))
  plot(test,main=paste(test_var_list[v],"\n",sep=""))
}
```

[How to interpret the plots]{.underline}:\
The diagnostic plots show residuals in four different ways\
\> **Residuals vs Fitted: Used to check the linear relationship
assumptions.** A horizontal line, without distinct patterns is an
indication for a linear relationship, what is good.\
\> Normal Q-Q: Used to examine whether the residuals are normally
distributed. It's good if residuals points follow the straight dashed
line.\
\> Scale-Location (or Spread-Location): Used to check the homogeneity of
variance of the residuals (homoscedasticity). Horizontal line with
equally spread points is a good indication of homoscedasticity.\
\> Residuals vs Leverage: Used to identify influential cases, that is
extreme values that might influence the regression results when included
or excluded from the analysis.

-   Is it linear? if it is not, what do your think we should do?

-   Which variables does follow a normal distribution?

-   Which variables does not follow a normal distribution?

-   What should we do? Transform the variables. Simplest e.g.
    Categorization

    ------------------------------------------------------------------------

## 4. If necessary, identify potential auxiliary variables

Auxiliary variables are variables in your data set that are either
correlated with a missing variable(s) (the recommendation is r \> 0.4)
or are believed to be associated with missingness. These are factors
that are not of particular interest in your analytic model , but they
are added to the imputation model to increase power and/or to help make
the assumption of MAR more plausible.

## 5. Interactions with sex

To impute correctly the missing values, we need to include the
interactions of our dataset variables. For instance, sex, age and bmi
are typically associated: a younger person tends to have a lower bmi
than an older person, and vice versa.

Thus, we must check the data interactions and then include them in the
multiple imputation.

R has the `interaction.plot()` function built-in.

-   x.factor = a factor variable whose levels will be on the X-axis.
    (aka. x-axis variable)
-   trace.factor = the second-factor variable whose levels will be
    represented as traces (aka. variable for lines).
-   response = a numeric response variable. (aka. y-axis variable)
-   fun = The function to compute the summary, e.g. median.
-   ylab = Y-axis label of the plot.
-   xlab = X-axis label of the plot.
-   trace.label = Label for the legend.
-   col = A vector of colors used for all traces.
-   lyt = A type of the lines drawn.
-   lwd = Width of the lines drawn.

```{r, results='hide', warning = FALSE}


#interaction.plot(x.factor = data$age, trace.factor = data$sex, response = data$bmi, fun = mean,  col = c("#0198f9", "#f95801")
#)

#temp = data %>% na.omit() %>% mutate(age_cat = cut(age, breaks = seq(30, 100, by = 10)))

#temp = data %>% mutate(age_cat = cut(age, breaks = seq(30, 100, by = 10)))

#interaction.plot(x.factor = temp$cat_age, trace.factor = temp$sex, response = temp$bmi, fun = mean,  col = c("#0198f9", "#f95801"))

#interaction.plot(data$sex, data$improve, data$bmi)

#interaction.plot(data$sex, data$improve, data$ohsdiff) 

interaction.plot(missdata$sex, missdata$improve, missdata$ohsdiff) 
```

## 6. Choosing imputation methods.

There are nine methods available for imputing a variable: **regress,
pmm, truncreg, intreg, logit, ologit, mlogit, poisson and nbreg**.

|  Method  |             Description              |     Scale type      | Default |
|:----------------:|:-----------------:|:----------------:|:----------------:|
|   pmm    |       Predictive mean matching       |       numeric       |    Y    |
|   norm   |      Bayesian linear regression      |       numeric       |         |
| norm.nob |   Linear regression, non-Bayesian    |       numeric       |         |
|   mean   |    Unconditional mean imputation     |       numeric       |         |
| 2L.norm  |        Two-level linear model        |       numeric       |         |
|  logreg  |         Logistic regression          |  factor, 2 levels   |    Y    |
| polyreg  |       Multinomial logit model        | factor, \>2 levels  |    Y    |
|   polr   |         Ordered logit model          | ordered, \>2 levels |    Y    |
|   lda    |     Linear discriminant analysis     |       factor        |         |
|  sample  | Random sample from the observed data |         any         |         |

-   Which variables we want to impute? EQ5D0, height, weight, bmi,
    satisfaction, ethnic, imdscore

**The order of the variables that we want to impute is key if we want to
use different methods:**

*Explanation of following three lines of the code*\
3.1 See order of the variables we want to impute\
3.2 Save the name of the variables ordered as they appear in the
dataset\
3.3 Include the method we want to use for imputing each variable
([remember]{.underline}: following the previous order)

```{r, results='hide', warning = FALSE}
names(data) #3.1
imp_list=c("eq5d0","height","weight","satisfaction","bmi","ethnic","imdscore")#3.2
methods=c("eq5d0","height","weight","satisfaction","bmi","ethnic","imdscore") #3.3

#####anyadir 
```

## 7. Imputations with mice (Multivariate Imputation by Chained Equations)

The **mice** package (*further details in*
[RDocumentation](https://www.rdocumentation.org/packages/mice/versions/3.14.0/topics/mice)
*or typing* help(mice) *in your console*) implements a method to
complete missing data based on Fully Conditional Specification, where
each incomplete variable is imputed by a separate model. Then name of
this function is the same than the package (i.e., `mice()` )

A simple version of the code for running the mice() function can looks
as following:\
**mice(data, maxit=10, m = 5)**

where:

-   data: your data frame
-   maxit: is the number of iterations for each imputation. The default
    is 5. It is important that the imputations for all variables reach
    convergence, otherwise they will be inaccurate.
-   m: number of multiple imputations (multiple datasets returned by the
    function). The default is 5. Generally the more the better, but then
    the analysis takes longer to run and it consume more computational
    resources. A thumb up rule regarding the percentage of missingness
    is commonly used: if there is a 10% of missing, include 10
    imputations; if there is a 20% missing, run 20 imputations, etc.

When using the mice package, the steps for your analysis are the
following:

1.  First your create your m imputed datasets. (mice() - function)

2.  Then you do your analysis on each of these datasets. (with() -
    function)

3.  In the end you combine these results together. (pool() - function)

    ![](images/Steps_MI.png)

## 8. Pooling results

The pool() function combines the estimates from m repeated complete data
analyses using Rubin's rules. The typical sequence of steps to do a
multiple imputation analysis is:

7.1 Impute the missing data by the mice function, resulting in a
multiple imputed data set (class mids);

7.2 Fit the model of interest (scientific model) on each imputed data
set by the with() function, resulting an object of class mira;\
*Classic statistical analysis has been implemented/are compatible to
with() function and can be called in the argument 'expr=' (for
expression), such as linear regression* (*lm()).*

7.3 Pool the estimates from each model into a single set of estimates
and standard errors, resulting is an object of class mipo;

Optionally, compare pooled estimates from different scientific models by
the pool.compare() function.

We create the impute datasets with the function `mice`:

```{r mice, results='hide', warning = FALSE}
library(mice)
imp = mice(data, maxit=10, m = 5)  #7.1
```

On the mice command line we can use the `m` option to specify the number
of imputations to be performed. In this example we chose 5 imputations.

```{r, results='hide', warning = FALSE}
fit = with(data=imp,expr = lm(ohsdiff~age+sex+ethnic+imdscore)) #7.2
summary(pool(fit), conf.int = TRUE) #7.3
```

## 9. Checking after imputation

```{r, results='hide', warning = FALSE}
# create the impute datasets
imp = mice(data, maxit=10, m = 5)  #7.1
fit = with(data=imp,expr = lm(ohsdiff~age+sex+ethnic+imdscore)) #7.2
summary(pool(fit), conf.int = TRUE) #7.3
```

## 10. What to do when a statistical analysis is not compatible with with() function

There might be cases where an analysis is such complex that you won't be
able to use the function with(). For this cases, the alternative is to
store the imputed datasets into a list, run the desired methods using a
loop, and later combine the results.

9.1 Run imputation\
9.2 Extract the imputed datasets: imp.data is a data.frame that contains
all the imputed datasets, which are distinguishable by the variable
.imp\
9.3 See de number of individuals for each imputed dataset\
9.4 Create a list for storing the results where the length is the number
if imputed datasets\
9.5 Run a loop for each of the imputed datasets and store the results in
the list created in the previous step.\
9.6 Transform the list into a mira object to perform the pooling\
9.7 Pool the results

```{r, results='hide', warning = FALSE}
imp = mice(data, maxit=10, m = 5)   #9.1
imp.data = complete(imp , "long")   #9.2
table(imp.data$.imp)                #9.3
Fit.list = vector("list", length(unique(imp.data$.imp))) #9.4
Fit.list2= vector("list", length(unique(imp.data$.imp))) #9.4
for (i in unique(imp.data$.imp)) {  #9.5 Loop start
      fit = lm(ohsdiff~age+sex+ethnic+imdscore, data = imp.data[imp.data$.imp == i,])
      Fit.list[[i]]= summary(fit)
                                  } #9.5
Fit.list.mira = as.mira(Fit.list)   #9.6 Loop end
summary(pool(Fit.list.mira),conf.int = TRUE) #9.7

```
